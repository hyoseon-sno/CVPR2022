{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aeda32f",
   "metadata": {},
   "source": [
    "# Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning\n",
    "\n",
    "https://github.com/Yfyangd/CVPR2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47031ed6",
   "metadata": {},
   "source": [
    "### 패키지 설치 \n",
    "\n",
    "keras==2.6.0 / \n",
    "numpy==1.19.5 / \n",
    "matplotlib==3.3.2 / \n",
    "PennyLane==0.19.0 / \n",
    "tensorflow==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881e8a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.6.0\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting numpy==1.19.5\n",
      "  Using cached numpy-1.19.5-cp39-cp39-win_amd64.whl (13.3 MB)\n",
      "Collecting matplotlib==3.3.2\n",
      "  Using cached matplotlib-3.3.2.tar.gz (37.9 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: PennyLane==0.19.0 in c:\\users\\hpclab\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.19.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.3.1 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.3.1\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d084ebc",
   "metadata": {},
   "source": [
    "### quantum_circuit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6feb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "n_qubits = 10\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights_0, weight_1):\n",
    "    qml.RX(inputs[0], wires=0)\n",
    "    qml.RX(inputs[1], wires=1)\n",
    "    qml.Rot(*weights_0, wires=0)\n",
    "    qml.RY(weight_1, wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cb805",
   "metadata": {},
   "source": [
    "### Self_Attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cc8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Conv2D, Layer, Activation\n",
    "\n",
    "class Self_Attention_Block(Layer):\n",
    "\n",
    "    def __init__(self, filters, ratio):\n",
    "        super(Self_Attention_Block, self).__init__()\n",
    "        self.conv0 = Conv2D(1, (1, 1), strides=(1, 1), padding='same',\n",
    "                           use_bias=False, activation=None)        \n",
    "        self.softmax = Activation('softmax')\n",
    "        self.conv1 = Conv2D(int(filters / ratio), (1, 1), strides=(1, 1), padding='same',\n",
    "                           use_bias=False, activation=None)\n",
    "        self.LN = LayerNormalization()\n",
    "        self.conv2 = Conv2D(int(filters), (1, 1), strides=(1, 1), padding='same',\n",
    "                           use_bias=False, activation=None)\n",
    "        self.relu = Activation('relu')\n",
    "        self.hard_sigmoid = Activation('hard_sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv0(inputs)\n",
    "        self_attention = self.softmax(x)\n",
    "        x = x * self_attention\n",
    "        x = self.relu(self.LN(self.conv1(x)))\n",
    "        excitation = self.hard_sigmoid(self.conv2(x))\n",
    "        x = inputs * excitation\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ec464",
   "metadata": {},
   "source": [
    "### Self_Proliferate_and_Attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2edbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, DepthwiseConv2D, Layer, Activation, add\n",
    "from Self_Attention import Self_Attention_Block\n",
    "from Self_Proliferate import Self_Proliferate_Block\n",
    "\n",
    "class Self_Proliferate_and_Attention_Block(Layer):\n",
    "\n",
    "    def __init__(self, dwkernel, strides, exp, out, ratio, use_se):\n",
    "        super(Self_Proliferate_and_Attention_Block, self).__init__()\n",
    "        self.strides = strides\n",
    "        self.use_se = use_se\n",
    "        self.conv = Conv2D(out, (1, 1), strides=(1, 1), padding='same',\n",
    "                           activation=None, use_bias=False)\n",
    "        self.relu = Activation('relu')\n",
    "        self.depthconv1 = DepthwiseConv2D(dwkernel, strides, padding='same', depth_multiplier=ratio-1,\n",
    "                                         activation=None, use_bias=False)\n",
    "        self.depthconv2 = DepthwiseConv2D(dwkernel, strides, padding='same', depth_multiplier=ratio-1,\n",
    "                                         activation=None, use_bias=False)\n",
    "        for i in range(5):\n",
    "            setattr(self, f\"batchnorm{i+1}\", BatchNormalization())\n",
    "        self.SAB1 = Self_Proliferate_Block(exp, ratio, 1, 3)\n",
    "        self.SAB2 = Self_Proliferate_Block(out, ratio, 1, 3)\n",
    "        self.se = Self_Attention_Block(exp, ratio)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.batchnorm1(self.depthconv1(inputs))\n",
    "        x = self.batchnorm2(self.conv(x))\n",
    "\n",
    "        y = self.relu(self.batchnorm3(self.SAB1(inputs)))\n",
    "        if self.strides > 1:\n",
    "            y = self.relu(self.batchnorm4(self.depthconv2(y)))\n",
    "        if self.use_se:\n",
    "            y = self.se(y)\n",
    "        y = self.batchnorm5(self.SAB2(y))\n",
    "        return add([x, y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddd4c9",
   "metadata": {},
   "source": [
    "### Self_Proliferate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afefd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Concatenate, DepthwiseConv2D, Layer, Activation\n",
    "from math import ceil\n",
    "\n",
    "class Self_Proliferate_Block(Layer):\n",
    "\n",
    "    def __init__(self, out, ratio, convkernel, dwkernel):\n",
    "        super(Self_Proliferate_Block, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.out = out\n",
    "        self.conv_out_channel = ceil(self.out * 1.0 / ratio)\n",
    "        self.conv = Conv2D(int(self.conv_out_channel), (convkernel, convkernel), use_bias=False,\n",
    "                           strides=(1, 1), padding='same', activation=None)\n",
    "        self.depthconv = DepthwiseConv2D(dwkernel, 1, padding='same', use_bias=False,\n",
    "                                         depth_multiplier=ratio-1, activation=None)\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        if self.ratio == 1:\n",
    "            return x\n",
    "        dw = self.depthconv(x)\n",
    "        dw = dw[:, :, :, :int(self.out - self.conv_out_channel)]\n",
    "        output = self.concat([x, dw])\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d7833",
   "metadata": {},
   "source": [
    "### CosineAnnealing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75939bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd0bb5",
   "metadata": {},
   "source": [
    "### CycleLoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53e7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.losses as kls # add kls code\n",
    "\n",
    "class CircleLoss(kls.Loss):\n",
    "    \n",
    "    def __init__(self,gamma: int = 64,margin: float = 0.25,batch_size: int = None,reduction='auto',name=None):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.gamma = gamma\n",
    "        self.margin = margin\n",
    "        self.O_p = 1 + self.margin\n",
    "        self.O_n = -self.margin\n",
    "        self.Delta_p = 1 - self.margin\n",
    "        self.Delta_n = self.margin\n",
    "        if batch_size:\n",
    "            self.batch_size = batch_size\n",
    "            self.batch_idxs = tf.expand_dims(tf.range(0, batch_size, dtype=tf.int32), 1)  # shape [batch,1]\n",
    "\n",
    "    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        alpha_p = tf.nn.relu(self.O_p - tf.stop_gradient(y_pred))\n",
    "        alpha_n = tf.nn.relu(tf.stop_gradient(y_pred) - self.O_n)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = (y_true * (alpha_p * (y_pred - self.Delta_p)) \n",
    "                  + (1 - y_true) * (alpha_n * (y_pred - self.Delta_n))\n",
    "                 ) * self.gamma\n",
    "        return tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5cf64",
   "metadata": {},
   "source": [
    "### Hybrid_CNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc08319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Conv2D, BatchNormalization, Reshape, Activation, add\n",
    "from Self_Proliferate_and_Attention import Self_Proliferate_and_Attention_Block\n",
    "from quantum_circuit import qnode\n",
    "\n",
    "class SPANet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, classes):\n",
    "        super(SPANet, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.img_input = keras.layers.Input(shape=(1, 512, 512, 3))\n",
    "        self.conv1 = Conv2D(16, (3, 3), strides=(2, 2), padding='same',\n",
    "                            activation=None, use_bias=False)\n",
    "        self.conv2 = Conv2D(960, (1, 1), strides=(1, 1), padding='same',\n",
    "                            activation=None, use_bias=False)\n",
    "        self.conv3 = Conv2D(1280, (1, 1), strides=(1, 1), padding='same',\n",
    "                            activation=None, use_bias=False)\n",
    "        self.conv4 = Conv2D(self.classes, (1, 1), strides=(1, 1), padding='same',\n",
    "                            activation=None, use_bias=False)\n",
    "\n",
    "        #Quantum Layer\n",
    "        self.weight_shapes = {\"weights_0\": 3, \"weight_1\": 1}\n",
    "        self.qlayer = qml.qnn.KerasLayer(qnode, self.weight_shapes, output_dim=self.classes)\n",
    "        \n",
    "        \n",
    "        for i in range(3):\n",
    "            setattr(self, f\"batchnorm{i+1}\", BatchNormalization())\n",
    "        self.relu = Activation('relu')\n",
    "        self.softmax = Activation('softmax')\n",
    "        self.pooling = GlobalAveragePooling2D()\n",
    "\n",
    "        self.dwkernels = [3, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5]\n",
    "        self.strides = [1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
    "        self.exps = [16, 48, 72, 72, 120, 240, 200, 184, 184, 480, 672, 672, 960, 960, 960, 960]\n",
    "        self.outs = [16, 24, 24, 40, 40, 80, 80, 80, 80, 112, 112, 160, 160, 160, 160, 160]\n",
    "        self.ratios = [2] * 16\n",
    "        self.use_sa = [False, False, False, True, True, False, False, False,\n",
    "                        False, True, True, True, False, True, False, True]\n",
    "        for i, args in enumerate(zip(self.dwkernels, self.strides, self.exps, self.outs, self.ratios, self.use_sa)):\n",
    "            setattr(self, f\"Self_Proliferate_and_Attention_Block{i}\", Self_Proliferate_and_Attention_Block(*args))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(inputs)))\n",
    "        # Iterate through Ghost Bottlenecks\n",
    "        for i in range(16):\n",
    "            x = getattr(self, f\"Self_Proliferate_and_Attention_Block{i}\")(x)\n",
    "        x = self.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = self.pooling(x)\n",
    "        x = Reshape((1, 1, int(x.shape[1])))(x)\n",
    "        x = self.relu(self.batchnorm3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = tf.squeeze(x, 1)\n",
    "        x = tf.squeeze(x, 1)\n",
    "        FC = tf.keras.layers.Dense(classes, activation=\"softmax\")\n",
    "        model = tf.keras.models.Sequential([x, self.qlayer, FC])\n",
    "        model = tf.keras.Model(self.img_input,x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b1d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
